{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category = Warning)\n",
    "\n",
    "# add parent folder path to the namespace\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# import modules and classes\n",
    "from utils.data_assets import PreProcessing, DataValidation\n",
    "import utils.global_paths as globpt\n",
    "import configurations as cnf\n",
    "\n",
    "# specify relative paths from global paths and create subfolders\n",
    "eval_path = os.path.join(globpt.data_path, 'validation')\n",
    "hist_path = os.path.join(eval_path, 'histograms')\n",
    "os.mkdir(eval_path) if not os.path.exists(eval_path) else None\n",
    "os.mkdir(hist_path) if not os.path.exists(hist_path) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessing()\n",
    "\n",
    "# load data from .csv files\n",
    "file_loc = os.path.join(globpt.data_path, 'SCADS_dataset.csv') \n",
    "df_adsorption = pd.read_csv(file_loc, sep=';', encoding = 'utf-8')\n",
    "file_loc = os.path.join(globpt.data_path, 'adsorbates_dataset.csv') \n",
    "df_adsorbates = pd.read_csv(file_loc, sep=';', encoding = 'utf-8')\n",
    "file_loc = os.path.join(globpt.data_path, 'adsorbents_dataset.csv') \n",
    "df_adsorbents = pd.read_csv(file_loc, sep=';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding physicochemical properties from guest species dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521920/521920 [00:04<00:00, 121251.71it/s]\n",
      "100%|██████████| 521920/521920 [00:05<00:00, 95152.30it/s] \n"
     ]
    }
   ],
   "source": [
    "# add molecular properties based on PUGCHEM API data\n",
    "print('Adding physicochemical properties from guest species dataset\\n')\n",
    "dataset = preprocessor.add_guest_properties(df_adsorption, df_adsorbates)\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# filter experiments leaving only valid uptake and pressure units, then convert \n",
    "# pressure and uptake to Pa (pressure) and mol/kg (uptake)\n",
    "# filter experiments by pressure and uptake units \n",
    "dataset = dataset[dataset[preprocessor.Q_unit_col].isin(preprocessor.valid_units)]\n",
    "\n",
    "# convert pressures to Pascal\n",
    "dataset[preprocessor.P_col] = dataset.progress_apply(lambda x : preprocessor.pressure_converter(x[preprocessor.P_unit_col], \n",
    "                                                                                                x['pressure']), \n",
    "                                                                                                axis = 1)\n",
    "# convert uptakes to mol/g\n",
    "dataset[preprocessor.Q_col] = dataset.progress_apply(lambda x : preprocessor.uptake_converter(x[preprocessor.Q_unit_col], \n",
    "                                                                                              x['adsorbed_amount'], \n",
    "                                                                                              x['mol_weight']), \n",
    "                                                                                              axis = 1)\n",
    "\n",
    "# further filter the dataset to remove experiments which values are outside desired boundaries, \n",
    "# such as experiments with negative temperature, pressure and uptake values\n",
    "dataset = dataset[dataset['temperature'].astype(int) > 0]\n",
    "dataset = dataset[dataset[preprocessor.P_col].astype(float).between(0.0, cnf.max_pressure)]\n",
    "dataset = dataset[dataset[preprocessor.Q_col].astype(float).between(0.0, cnf.max_uptake)]\n",
    "\n",
    "# Aggregate values using groupby function in order to group the dataset by experiments\n",
    "aggregate_dict = {'temperature' : 'first',                  \n",
    "                  'adsorbent_name' : 'first',\n",
    "                  'adsorbates_name' : 'first',                  \n",
    "                  'complexity' : 'first',                  \n",
    "                  'mol_weight' : 'first',\n",
    "                  'covalent_units' : 'first',\n",
    "                  'H_acceptors' : 'first',\n",
    "                  'H_donors' : 'first',\n",
    "                  'heavy_atoms' : 'first', \n",
    "                  'pressure_in_Pascal' : list,\n",
    "                  'uptake_in_mol_g' : list}\n",
    "   \n",
    "# group dataset by experiments and drop filename column as it is not necessary\n",
    "dataset_grouped = dataset.groupby('filename', as_index=False).agg(aggregate_dict)\n",
    "dataset_grouped.drop(columns='filename', axis=1, inplace=True)\n",
    "\n",
    "# remove series of pressure/uptake with less than X points, drop rows containing nan\n",
    "# values and select a subset of samples for training\n",
    "dataset_grouped = dataset_grouped[~dataset_grouped[preprocessor.P_col].apply(lambda x: all(elem == 0 for elem in x))]\n",
    "dataset_grouped = dataset_grouped[dataset_grouped[preprocessor.P_col].apply(lambda x: len(x)) >= cnf.min_points]\n",
    "dataset_grouped = dataset_grouped.dropna()\n",
    "\n",
    "# check to avoid errors when selecting number of samples higher than effectively \n",
    "# available samples. If less are available, the entire dataset is selected\n",
    "if cnf.num_samples < dataset_grouped.shape[0]:\n",
    "    dataset_grouped = dataset_grouped.sample(n=cnf.num_samples, random_state=30).reset_index()\n",
    "\n",
    "# preprocess sequences to remove leading 0 values (some experiments may have several\n",
    "# zero measurements at the start), make sure that every experiment starts with pressure\n",
    "# of 0 Pa and uptake of 0 mol/g (effectively converges to zero)\n",
    "dataset_grouped[[preprocessor.P_col, preprocessor.Q_col]] = dataset_grouped.apply(lambda row: \n",
    "                 preprocessor.remove_leading_zeros(row[preprocessor.P_col],\n",
    "                 row[preprocessor.Q_col]), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 General validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of adsorption measurements:   440663\n",
      "Number of unique experiments:        21044\n",
      "Number of dataset features:          11\n",
      "Average measurements per experiment: 20\n",
      "\n",
      "Checking for missing values in the dataset:\n",
      "\n",
      "No columns with missing values\n",
      "\n",
      "\n",
      "Generating histograms for the grouped dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.74it/s]\n"
     ]
    }
   ],
   "source": [
    "validator = DataValidation()\n",
    "\n",
    "# print report with statistics and info about the non-grouped dataset\n",
    "print(f'''\n",
    "Number of adsorption measurements:   {dataset.shape[0]}\n",
    "Number of unique experiments:        {dataset_grouped.shape[0]}\n",
    "Number of dataset features:          {dataset_grouped.shape[1]}\n",
    "Average measurements per experiment: {dataset.shape[0]//dataset_grouped.shape[0]}\n",
    "''')\n",
    "\n",
    "# perform prelimiary analysis on the grouped, unsplit dataset\n",
    "# check columns with null values\n",
    "print('Checking for missing values in the dataset:\\n')\n",
    "missing_values = validator.check_missing_values(dataset_grouped)  \n",
    "\n",
    "# generate histograms of the grouped dataset features (only those that are continuous)\n",
    "print('\\nGenerating histograms for the grouped dataset\\n')\n",
    "validator.plot_histograms(dataset_grouped, eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Validation of dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation best random seed for data splitting\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 308/500 [01:04<00:39,  4.81it/s]"
     ]
    }
   ],
   "source": [
    "# validate splitting based on random seed\n",
    "print('\\nValidation best random seed for data splitting\\n')\n",
    "min_diff, best_seed, best_split = validator.data_split_validation(dataset, cnf.test_size, 500)\n",
    "print(f'''\\nBest split found with split_seed of {best_seed}, with total difference equal to {round(min_diff, 3)}\n",
    "Mean and standard deviation differences per features (X and Y):''')\n",
    "for key, val in best_split.items():\n",
    "    print(f'{key} ---> mean difference = {val[0]}, STD difference = {val[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aquarius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
